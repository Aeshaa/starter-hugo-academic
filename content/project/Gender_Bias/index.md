---
# Documentation: https://wowchemy.com/docs/managing-content/

title: "Fairness in Machine Learning: Detecting and Removing Gender Bias in Language Models"
summary: "Addressing gender bias in language models by implementing and testing two debiasing approaches."
authors: []
tags: 
  - Research
  # - Machine Learning

categories: []
date:

# Optional external URL for project (replaces project detail page).
external_link: ""

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ""
  focal_point: "Right"
  preview_only: false

# Custom links (optional).
#   Uncomment and edit lines below to show custom links.
# links:
# - name: Follow
#   url: https://twitter.com
#   icon_pack: fab
#   icon: twitter

url_code: ""
url_pdf: "uploads/Fairness Gender Bias.pdf"
url_slides: ""
url_video: ""

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides: ""
---
### Introduction

This study addresses gender biases in language models by implementing and testing two approaches: debiasing the dataset and debiasing the corpus embeddings (hard debiasing), resulting in significant reductions of bias in the BERT model, with dataset debiasing reducing bias by 29.41% and hard debiasing by 11.76%, accompanied by minimal classification accuracy loss.




