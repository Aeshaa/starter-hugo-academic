---
# Documentation: https://wowchemy.com/docs/managing-content/

title: "Explainable AI for Adversarially Trained Deep-Learning Models"
summary: "Designing a visualization system for training models that resist adversarial attacks and remain interpretable."
authors: []
tags: 
  - Research
  # - Machine Learning

categories: []
date:

# Optional external URL for project (replaces project detail page).
external_link: ""

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ""
  focal_point: "BottomRight"
  preview_only: false

# Custom links (optional).
#   Uncomment and edit lines below to show custom links.
# links:
# - name: Follow
#   url: https://twitter.com
#   icon_pack: fab
#   icon: twitter

url_code: ""
url_pdf: ""
url_slides: ""
url_video: ""

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides: ""
---

Deep learning models have demonstrated remarkable performance in various applications, particularly in computer vision tasks. 

However, these models are vulnerable to adversarial attacks, which can significantly compromise their performance. To address this, there have been attempts to incorporate adversarial signals during model training. 

Unfortunately, doing so often results in reduced model interpretability. 

This project proposes the development of a visualization system that enables deep learning models to resist adversarial attacks while maintaining their interpretability using concepts.
